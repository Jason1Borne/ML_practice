{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAL_kaggle_to_subject_final_edited.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jason1Borne/ML_practice/blob/master/GAL_kaggle_to_subject_final_edited.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzfJyWo4OYCg",
        "colab_type": "text"
      },
      "source": [
        "#Import kaggle.json to download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF-t3GClgybk",
        "colab_type": "code",
        "outputId": "30f1be11-d4bb-46bd-e8c0-2779e21f5f81",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 96
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-772e5985-e6c3-411a-9728-0dfa80098b5a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-772e5985-e6c3-411a-9728-0dfa80098b5a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"szutang\",\"key\":\"a789eb8084078329e99d1ba0960540c5\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at2TFfHdhEU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVQpoAh4hhFN",
        "colab_type": "code",
        "outputId": "928eea21-e43f-42cb-990e-7a4b6572dba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!kaggle competitions download -c grasp-and-lift-eeg-detection"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/5.16M [00:00<?, ?B/s]\r 97% 5.00M/5.16M [00:00<00:00, 34.0MB/s]\n",
            "100% 5.16M/5.16M [00:00<00:00, 33.1MB/s]\n",
            "Downloading test.zip to /content\n",
            " 90% 137M/153M [00:00<00:00, 127MB/s]\n",
            "100% 153M/153M [00:01<00:00, 150MB/s]\n",
            "Downloading train.zip to /content\n",
            " 98% 894M/915M [00:09<00:00, 92.6MB/s]\n",
            "100% 915M/915M [00:09<00:00, 102MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anjyXBafiMRF",
        "colab_type": "code",
        "outputId": "4e068bac-faf8-4f29-c564-6ce2755cb9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!unzip train.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.zip\n",
            "   creating: train/\n",
            "  inflating: train/subj10_series1_data.csv  \n",
            "  inflating: train/subj10_series1_events.csv  \n",
            "  inflating: train/subj10_series2_data.csv  \n",
            "  inflating: train/subj10_series2_events.csv  \n",
            "  inflating: train/subj10_series3_data.csv  \n",
            "  inflating: train/subj10_series3_events.csv  \n",
            "  inflating: train/subj10_series4_data.csv  \n",
            "  inflating: train/subj10_series4_events.csv  \n",
            "  inflating: train/subj10_series5_data.csv  \n",
            "  inflating: train/subj10_series5_events.csv  \n",
            "  inflating: train/subj10_series6_data.csv  \n",
            "  inflating: train/subj10_series6_events.csv  \n",
            "  inflating: train/subj10_series7_data.csv  \n",
            "  inflating: train/subj10_series7_events.csv  \n",
            "  inflating: train/subj10_series8_data.csv  \n",
            "  inflating: train/subj10_series8_events.csv  \n",
            "  inflating: train/subj11_series1_data.csv  \n",
            "  inflating: train/subj11_series1_events.csv  \n",
            "  inflating: train/subj11_series2_data.csv  \n",
            "  inflating: train/subj11_series2_events.csv  \n",
            "  inflating: train/subj11_series3_data.csv  \n",
            "  inflating: train/subj11_series3_events.csv  \n",
            "  inflating: train/subj11_series4_data.csv  \n",
            "  inflating: train/subj11_series4_events.csv  \n",
            "  inflating: train/subj11_series5_data.csv  \n",
            "  inflating: train/subj11_series5_events.csv  \n",
            "  inflating: train/subj11_series6_data.csv  \n",
            "  inflating: train/subj11_series6_events.csv  \n",
            "  inflating: train/subj11_series7_data.csv  \n",
            "  inflating: train/subj11_series7_events.csv  \n",
            "  inflating: train/subj11_series8_data.csv  \n",
            "  inflating: train/subj11_series8_events.csv  \n",
            "  inflating: train/subj12_series1_data.csv  \n",
            "  inflating: train/subj12_series1_events.csv  \n",
            "  inflating: train/subj12_series2_data.csv  \n",
            "  inflating: train/subj12_series2_events.csv  \n",
            "  inflating: train/subj12_series3_data.csv  \n",
            "  inflating: train/subj12_series3_events.csv  \n",
            "  inflating: train/subj12_series4_data.csv  \n",
            "  inflating: train/subj12_series4_events.csv  \n",
            "  inflating: train/subj12_series5_data.csv  \n",
            "  inflating: train/subj12_series5_events.csv  \n",
            "  inflating: train/subj12_series6_data.csv  \n",
            "  inflating: train/subj12_series6_events.csv  \n",
            "  inflating: train/subj12_series7_data.csv  \n",
            "  inflating: train/subj12_series7_events.csv  \n",
            "  inflating: train/subj12_series8_data.csv  \n",
            "  inflating: train/subj12_series8_events.csv  \n",
            "  inflating: train/subj1_series1_data.csv  \n",
            "  inflating: train/subj1_series1_events.csv  \n",
            "  inflating: train/subj1_series2_data.csv  \n",
            "  inflating: train/subj1_series2_events.csv  \n",
            "  inflating: train/subj1_series3_data.csv  \n",
            "  inflating: train/subj1_series3_events.csv  \n",
            "  inflating: train/subj1_series4_data.csv  \n",
            "  inflating: train/subj1_series4_events.csv  \n",
            "  inflating: train/subj1_series5_data.csv  \n",
            "  inflating: train/subj1_series5_events.csv  \n",
            "  inflating: train/subj1_series6_data.csv  \n",
            "  inflating: train/subj1_series6_events.csv  \n",
            "  inflating: train/subj1_series7_data.csv  \n",
            "  inflating: train/subj1_series7_events.csv  \n",
            "  inflating: train/subj1_series8_data.csv  \n",
            "  inflating: train/subj1_series8_events.csv  \n",
            "  inflating: train/subj2_series1_data.csv  \n",
            "  inflating: train/subj2_series1_events.csv  \n",
            "  inflating: train/subj2_series2_data.csv  \n",
            "  inflating: train/subj2_series2_events.csv  \n",
            "  inflating: train/subj2_series3_data.csv  \n",
            "  inflating: train/subj2_series3_events.csv  \n",
            "  inflating: train/subj2_series4_data.csv  \n",
            "  inflating: train/subj2_series4_events.csv  \n",
            "  inflating: train/subj2_series5_data.csv  \n",
            "  inflating: train/subj2_series5_events.csv  \n",
            "  inflating: train/subj2_series6_data.csv  \n",
            "  inflating: train/subj2_series6_events.csv  \n",
            "  inflating: train/subj2_series7_data.csv  \n",
            "  inflating: train/subj2_series7_events.csv  \n",
            "  inflating: train/subj2_series8_data.csv  \n",
            "  inflating: train/subj2_series8_events.csv  \n",
            "  inflating: train/subj3_series1_data.csv  \n",
            "  inflating: train/subj3_series1_events.csv  \n",
            "  inflating: train/subj3_series2_data.csv  \n",
            "  inflating: train/subj3_series2_events.csv  \n",
            "  inflating: train/subj3_series3_data.csv  \n",
            "  inflating: train/subj3_series3_events.csv  \n",
            "  inflating: train/subj3_series4_data.csv  \n",
            "  inflating: train/subj3_series4_events.csv  \n",
            "  inflating: train/subj3_series5_data.csv  \n",
            "  inflating: train/subj3_series5_events.csv  \n",
            "  inflating: train/subj3_series6_data.csv  \n",
            "  inflating: train/subj3_series6_events.csv  \n",
            "  inflating: train/subj3_series7_data.csv  \n",
            "  inflating: train/subj3_series7_events.csv  \n",
            "  inflating: train/subj3_series8_data.csv  \n",
            "  inflating: train/subj3_series8_events.csv  \n",
            "  inflating: train/subj4_series1_data.csv  \n",
            "  inflating: train/subj4_series1_events.csv  \n",
            "  inflating: train/subj4_series2_data.csv  \n",
            "  inflating: train/subj4_series2_events.csv  \n",
            "  inflating: train/subj4_series3_data.csv  \n",
            "  inflating: train/subj4_series3_events.csv  \n",
            "  inflating: train/subj4_series4_data.csv  \n",
            "  inflating: train/subj4_series4_events.csv  \n",
            "  inflating: train/subj4_series5_data.csv  \n",
            "  inflating: train/subj4_series5_events.csv  \n",
            "  inflating: train/subj4_series6_data.csv  \n",
            "  inflating: train/subj4_series6_events.csv  \n",
            "  inflating: train/subj4_series7_data.csv  \n",
            "  inflating: train/subj4_series7_events.csv  \n",
            "  inflating: train/subj4_series8_data.csv  \n",
            "  inflating: train/subj4_series8_events.csv  \n",
            "  inflating: train/subj5_series1_data.csv  \n",
            "  inflating: train/subj5_series1_events.csv  \n",
            "  inflating: train/subj5_series2_data.csv  \n",
            "  inflating: train/subj5_series2_events.csv  \n",
            "  inflating: train/subj5_series3_data.csv  \n",
            "  inflating: train/subj5_series3_events.csv  \n",
            "  inflating: train/subj5_series4_data.csv  \n",
            "  inflating: train/subj5_series4_events.csv  \n",
            "  inflating: train/subj5_series5_data.csv  \n",
            "  inflating: train/subj5_series5_events.csv  \n",
            "  inflating: train/subj5_series6_data.csv  \n",
            "  inflating: train/subj5_series6_events.csv  \n",
            "  inflating: train/subj5_series7_data.csv  \n",
            "  inflating: train/subj5_series7_events.csv  \n",
            "  inflating: train/subj5_series8_data.csv  \n",
            "  inflating: train/subj5_series8_events.csv  \n",
            "  inflating: train/subj6_series1_data.csv  \n",
            "  inflating: train/subj6_series1_events.csv  \n",
            "  inflating: train/subj6_series2_data.csv  \n",
            "  inflating: train/subj6_series2_events.csv  \n",
            "  inflating: train/subj6_series3_data.csv  \n",
            "  inflating: train/subj6_series3_events.csv  \n",
            "  inflating: train/subj6_series4_data.csv  \n",
            "  inflating: train/subj6_series4_events.csv  \n",
            "  inflating: train/subj6_series5_data.csv  \n",
            "  inflating: train/subj6_series5_events.csv  \n",
            "  inflating: train/subj6_series6_data.csv  \n",
            "  inflating: train/subj6_series6_events.csv  \n",
            "  inflating: train/subj6_series7_data.csv  \n",
            "  inflating: train/subj6_series7_events.csv  \n",
            "  inflating: train/subj6_series8_data.csv  \n",
            "  inflating: train/subj6_series8_events.csv  \n",
            "  inflating: train/subj7_series1_data.csv  \n",
            "  inflating: train/subj7_series1_events.csv  \n",
            "  inflating: train/subj7_series2_data.csv  \n",
            "  inflating: train/subj7_series2_events.csv  \n",
            "  inflating: train/subj7_series3_data.csv  \n",
            "  inflating: train/subj7_series3_events.csv  \n",
            "  inflating: train/subj7_series4_data.csv  \n",
            "  inflating: train/subj7_series4_events.csv  \n",
            "  inflating: train/subj7_series5_data.csv  \n",
            "  inflating: train/subj7_series5_events.csv  \n",
            "  inflating: train/subj7_series6_data.csv  \n",
            "  inflating: train/subj7_series6_events.csv  \n",
            "  inflating: train/subj7_series7_data.csv  \n",
            "  inflating: train/subj7_series7_events.csv  \n",
            "  inflating: train/subj7_series8_data.csv  \n",
            "  inflating: train/subj7_series8_events.csv  \n",
            "  inflating: train/subj8_series1_data.csv  \n",
            "  inflating: train/subj8_series1_events.csv  \n",
            "  inflating: train/subj8_series2_data.csv  \n",
            "  inflating: train/subj8_series2_events.csv  \n",
            "  inflating: train/subj8_series3_data.csv  \n",
            "  inflating: train/subj8_series3_events.csv  \n",
            "  inflating: train/subj8_series4_data.csv  \n",
            "  inflating: train/subj8_series4_events.csv  \n",
            "  inflating: train/subj8_series5_data.csv  \n",
            "  inflating: train/subj8_series5_events.csv  \n",
            "  inflating: train/subj8_series6_data.csv  \n",
            "  inflating: train/subj8_series6_events.csv  \n",
            "  inflating: train/subj8_series7_data.csv  \n",
            "  inflating: train/subj8_series7_events.csv  \n",
            "  inflating: train/subj8_series8_data.csv  \n",
            "  inflating: train/subj8_series8_events.csv  \n",
            "  inflating: train/subj9_series1_data.csv  \n",
            "  inflating: train/subj9_series1_events.csv  \n",
            "  inflating: train/subj9_series2_data.csv  \n",
            "  inflating: train/subj9_series2_events.csv  \n",
            "  inflating: train/subj9_series3_data.csv  \n",
            "  inflating: train/subj9_series3_events.csv  \n",
            "  inflating: train/subj9_series4_data.csv  \n",
            "  inflating: train/subj9_series4_events.csv  \n",
            "  inflating: train/subj9_series5_data.csv  \n",
            "  inflating: train/subj9_series5_events.csv  \n",
            "  inflating: train/subj9_series6_data.csv  \n",
            "  inflating: train/subj9_series6_events.csv  \n",
            "  inflating: train/subj9_series7_data.csv  \n",
            "  inflating: train/subj9_series7_events.csv  \n",
            "  inflating: train/subj9_series8_data.csv  \n",
            "  inflating: train/subj9_series8_events.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGxG1YSbipis",
        "colab_type": "code",
        "outputId": "2356a6de-d8cd-4071-bf66-0dd7dcf438b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "!unzip test.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test.zip\n",
            "   creating: test/\n",
            "  inflating: test/subj10_series10_data.csv  \n",
            "  inflating: test/subj10_series9_data.csv  \n",
            "  inflating: test/subj11_series10_data.csv  \n",
            "  inflating: test/subj11_series9_data.csv  \n",
            "  inflating: test/subj12_series10_data.csv  \n",
            "  inflating: test/subj12_series9_data.csv  \n",
            "  inflating: test/subj1_series10_data.csv  \n",
            "  inflating: test/subj1_series9_data.csv  \n",
            "  inflating: test/subj2_series10_data.csv  \n",
            "  inflating: test/subj2_series9_data.csv  \n",
            "  inflating: test/subj3_series10_data.csv  \n",
            "  inflating: test/subj3_series9_data.csv  \n",
            "  inflating: test/subj4_series10_data.csv  \n",
            "  inflating: test/subj4_series9_data.csv  \n",
            "  inflating: test/subj5_series10_data.csv  \n",
            "  inflating: test/subj5_series9_data.csv  \n",
            "  inflating: test/subj6_series10_data.csv  \n",
            "  inflating: test/subj6_series9_data.csv  \n",
            "  inflating: test/subj7_series10_data.csv  \n",
            "  inflating: test/subj7_series9_data.csv  \n",
            "  inflating: test/subj8_series10_data.csv  \n",
            "  inflating: test/subj8_series9_data.csv  \n",
            "  inflating: test/subj9_series10_data.csv  \n",
            "  inflating: test/subj9_series9_data.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecqzqwzYxfb-",
        "colab_type": "code",
        "outputId": "d67b086c-682b-48de-8701-863101044dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!unzip sample_submission.csv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC0fn7M1O5HM",
        "colab_type": "text"
      },
      "source": [
        "#Import Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaVkum2ZOpNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6dAJpdcPBUh",
        "colab_type": "text"
      },
      "source": [
        "#First, train the model on all subjects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpHpNBUAw2eD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gc\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawq1IiiQ-OP",
        "colab_type": "text"
      },
      "source": [
        " read dataset according to file path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jruxloBRQNUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_files_names = []\n",
        "Y_train_files_names = []\n",
        "for subj in range(1, 13):\n",
        "    for series in range(1, 8):\n",
        "        X_train_files_names.append('./train/subj'+str(subj)+'_series'+str(series)+'_data.csv')\n",
        "\n",
        "        Y_train_files_names.append('./train/subj'+str(subj)+'_series'+str(series)+'_events.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGcFaItNQbc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val_files_names = []\n",
        "Y_val_files_names = []\n",
        "for subj in range(1, 13):\n",
        "    series = 8\n",
        "    X_val_files_names.append('./train/subj'+str(subj)+'_series'+str(series)+'_data.csv')\n",
        "\n",
        "    Y_val_files_names.append('./train/subj'+str(subj)+'_series'+str(series)+'_events.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrh4MeE4Qbl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = pd.DataFrame()\n",
        "\n",
        "for file_path in X_train_files_names:\n",
        "    x = pd.read_csv(file_path)\n",
        "    X_train = pd.concat([X_train, x])\n",
        "\n",
        "X_val = pd.DataFrame()\n",
        "\n",
        "for file_path in X_val_files_names:\n",
        "    x = pd.read_csv(file_path)\n",
        "    X_val = pd.concat([X_val, x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4tC0TJCRPit",
        "colab_type": "text"
      },
      "source": [
        "check memory status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9sew15KQbxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!free"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHQQxhbARWNE",
        "colab_type": "text"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxZI9alHQbuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.drop('id', axis=1, inplace=True)\n",
        "X_val.drop('id', axis=1, inplace=True)\n",
        "\n",
        "def preprocess(col):\n",
        "    \n",
        "    mean = col.mean()\n",
        "    std = col.std()\n",
        "    col -= mean\n",
        "    col /=std\n",
        "    return col\n",
        "\n",
        "X_train = X_train.apply(preprocess)\n",
        "X_val = X_val.apply(preprocess)\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KFPEr68QbsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.values\n",
        "X_val = X_val.values\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWgVRpwIbPM2",
        "colab_type": "text"
      },
      "source": [
        "read labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCYBUwxsQbqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = pd.DataFrame()\n",
        "for file_path in Y_train_files_names:\n",
        "    y = pd.read_csv(file_path)\n",
        "    Y_train_all = pd.concat([Y_train, y])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYlheiagbN8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_val = pd.DataFrame()\n",
        "for file_path in Y_val_files_names:\n",
        "    y = pd.read_csv(file_path)\n",
        "    Y_val = pd.concat([Y_val, y])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSTJfmMlbOK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train.drop('id', axis=1, inplace=True)\n",
        "Y_val.drop('id', axis=1, inplace=True)\n",
        "Y_train = Y_train.values\n",
        "Y_val = Y_val.values\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfzRsUFWbOHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train_std.shape[0], Y_train_all.shape[0], X_val.shape[0], Y_val.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46v_j4SLbfkN",
        "colab_type": "text"
      },
      "source": [
        "Prepare the train data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S5-R6uIbOFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#训练数据集的generator\n",
        "time_interval = 1024#原数据是500Hz, 取1000个点就是2s的数据\n",
        "subsamples = 4#每间隔100采样点再踩一次样,Tx=30\n",
        "\n",
        "def generator(batch_size):\n",
        "    while 1:\n",
        "        x = np.zeros((batch_size, time_interval//subsamples, 32))\n",
        "        y = []\n",
        "        index = np.random.randint(0, len(X_train)-time_interval-16*batch_size)\n",
        "        indexes = range(index, index + 16*batch_size, 16)\n",
        "        for i , index in enumerate(indexes):\n",
        "            \n",
        "            x[i] = X_train[index : index+time_interval : subsamples]\n",
        "            y.append(Y_train[index+time_interval])\n",
        "        #x = x.reshape(x.shape[0], x.shape[1], x.shape[2],1)    \n",
        "        y = np.asarray(y)\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9BzVwV2b13U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import AveragePooling1D,LeakyReLU, Lambda, Dense, LSTM, Dropout, CuDNNLSTM, BatchNormalization, Conv2D, Flatten, MaxPooling2D\n",
        "import numpy as np\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import  Dense, LSTM, Dropout, CuDNNLSTM, Conv1D, BatchNormalization, Conv2D, Flatten, MaxPooling2D, MaxPooling1D\n",
        "\n",
        "savemodel = ModelCheckpoint(filepath='Conv1D.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is0rXZqUc2It",
        "colab_type": "text"
      },
      "source": [
        "validation generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u51EXfhcv8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "#validation generator\n",
        "def val_generator(batch_size):\n",
        "    while 1:\n",
        "        x = np.zeros((batch_size, time_interval//subsamples, 32))\n",
        "        y = []\n",
        "        index = np.random.randint(0, len(X_val)-time_interval-16*batch_size)\n",
        "        indexes = range(index, index + 16*batch_size, 16)\n",
        "        for i , index in enumerate(indexes):\n",
        "            \n",
        "            x[i] = X_val[index : index+time_interval : subsamples]\n",
        "            y.append(Y_val[index+time_interval])\n",
        "        #x = x.reshape(x.shape[0], x.shape[1], x.shape[2],1)    \n",
        "        y = np.asarray(y)\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-nBT1TIb8MP",
        "colab_type": "text"
      },
      "source": [
        "#Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnIe_UJYcOh8",
        "colab_type": "text"
      },
      "source": [
        "First model： 非常简单的一个模型，直接用1D来处理数据，但是最后结果还不错，针对每一个subj进行训练过后，平均AUC 得分能有0.9之上"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES7_jr5_bODQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "savemodel = ModelCheckpoint(filepath='Conv1D.h5')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, strides=1, padding='valid', activation = 'relu', input_shape=[time_interval//subsamples, 32]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling1D(pool_size=2, stride=2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(124, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "adam = Adam(lr = 0.001)\n",
        "\n",
        "model.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOERLY4cRpH",
        "colab_type": "text"
      },
      "source": [
        "Second model：排名第三用的一种模型，直接输入所有的原数据， window_size = 4096, sub_sample = 1, 让模型自己学习如何采样"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz7iSf0tbOAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "savemodel = ModelCheckpoint(filepath='new_model_Conv1D.h5')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=6, kernel_size=5, strides=1, padding='same', input_shape=[time_interval//subsamples, 32]))\n",
        "model.add(Conv1D(filters=6, kernel_size=16, strides=16, padding='valid'))\n",
        "model.add(Conv1D(filters=32, kernel_size=7, padding='same', strides=1))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(AveragePooling1D(pool_size=3, strides=2))\n",
        "model.add(Conv1D(filters=32, kernel_size=5, padding='same', strides=1))\n",
        "model.add(AveragePooling1D(pool_size=12, strides=8))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(CuDNNLSTM(64))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "adam = Adam(lr = 0.001)\n",
        "\n",
        "model.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvQZ0W8ffiJy",
        "colab_type": "text"
      },
      "source": [
        "Third model，自己想的模型，用LSTM来学习，效果未知"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSkHAiIhfhcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "savemodel = ModelCheckpoint(filepath='keras_model/new_model_LSTM.h5')\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(128, return_sequences=True, input_shape=[30, 32]))\n",
        "model.add(CuDNNLSTM(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "adam = Adam(lr = 0.001)\n",
        "#parallel_model = multi_gpu_model(model, gpus=8)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j6NCzVEgAuy",
        "colab_type": "text"
      },
      "source": [
        "Forth model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJC51GyagDXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "savemodel = ModelCheckpoint(filepath='keras_model/new_model_Conv2D.h5')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = \"same\", activation = \"relu\", input_shape = (time_interval//subsamples, 32, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation = \"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(6, activation = \"sigmoid\"))\n",
        "adam = Adam(lr = 0.001)\n",
        "\n",
        "model.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLuWUcsTgIjC",
        "colab_type": "text"
      },
      "source": [
        "Fith model:当作时空热力图来进行处理，使用2D卷积"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-3E8caqgLG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "#model.add(CuDNNLSTM(128, input_shape = (time_steps//subsample, 32)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (7,7), padding = \"same\", activation = \"relu\", input_shape = (time_interval//subsamples, 32, 1)))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (5,5), padding = \"same\", activation = \"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
        "model.add(Flatten())\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation = \"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "model.add(Dense(6, activation = \"sigmoid\"))\n",
        "\n",
        "adam = Adam(lr = 0.001)\n",
        "\n",
        "model.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRfx3hjWQbjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 2000\n",
        "steps_per_epcoh = X_train_std.shape[0]//batch_size\n",
        "\n",
        "model.fit_generator(generator(batch_size), steps_per_epoch = steps_per_epcoh ,epochs=1, callbacks=[savemodel], validation_data=val_generator(64), validation_steps=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAWK0SsrdSLQ",
        "colab_type": "text"
      },
      "source": [
        "validation AUC score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji6T56mDcpjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#validation\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "scores = []\n",
        "num_batch = 10\n",
        "batch_size = 256\n",
        "\n",
        "for i in range(num_batch):\n",
        "    x_val = np.zeros((batch_size, time_interval//subsamples, 32))\n",
        "    y_val = []\n",
        "    for i in range(batch_size):\n",
        "        index = np.random.randint(0, X_val.shape[0]-time_interval)\n",
        "        x_val[i] = X_val[index : index+time_interval : subsamples]\n",
        "        y_val.append(Y_val[index+time_interval])\n",
        "    y_val = np.asarray(y_val)\n",
        "    #x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2],1)   \n",
        "    y_out = model.predict(x_val)\n",
        "    score = roc_auc_score(y_val, y_out)\n",
        "    scores.append(score)\n",
        "scores = np.asarray(scores)\n",
        "print(np.mean(scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awinks8HdwAj",
        "colab_type": "text"
      },
      "source": [
        "#Start to train the model on individual subject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37VkjwFvd8mW",
        "colab_type": "text"
      },
      "source": [
        "load data according to subj"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BplM5_4w6X8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_files_names = []\n",
        "Y_train_files_names = []\n",
        "for subj in range(1, 13):\n",
        "    x_subjs = []\n",
        "    y_subjs = []\n",
        "    for series in range(1, 8):\n",
        "        x_subjs.append('./train/subj'+str(subj)+'_series'+str(series)+'_data.csv')\n",
        "\n",
        "        y_subjs.append('./train/subj'+str(subj)+'_series'+str(series)+'_events.csv')\n",
        "    X_train_files_names.append(x_subjs)\n",
        "    Y_train_files_names.append(y_subjs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUlgKH4NTcVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_val_files_names = []\n",
        "Y_val_files_names = []\n",
        "for subj in range(1, 13):\n",
        "    x_subjs = []\n",
        "    y_subjs = []\n",
        "    series = 8\n",
        "    x_subjs .append('./train/subj'+str(subj)+'_series'+str(series)+'_data.csv')\n",
        "\n",
        "    y_subjs.append('./train/subj'+str(subj)+'_series'+str(series)+'_events.csv')\n",
        "    X_val_files_names.append(x_subjs)\n",
        "    Y_val_files_names.append(y_subjs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYK0CbYmrn3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDN3KuhGsVE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(col):\n",
        "    \n",
        "    mean = col.mean()\n",
        "    std = col.std()\n",
        "    col -= mean\n",
        "    col /=std\n",
        "    return col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH-1enK5eYFU",
        "colab_type": "text"
      },
      "source": [
        "For each subj, prepare the train.csv and val.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB989nskrAS5",
        "colab_type": "code",
        "outputId": "7bbe7689-ca42-4432-ef87-24807b280772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(12):\n",
        "    \n",
        "    X_train = pd.DataFrame()\n",
        "    \n",
        "    for file_path in X_train_files_names[i]:\n",
        "        x = pd.read_csv(file_path)\n",
        "        X_train = pd.concat([X_train, x])\n",
        "    \n",
        "    X_train.drop('id', axis=1, inplace=True)\n",
        "    print('load X_train'+str(i)+'success')\n",
        "    \n",
        "    X_val = pd.DataFrame()\n",
        "    \n",
        "    for file_path in X_val_files_names[i]:\n",
        "        x = pd.read_csv(file_path)\n",
        "        X_val = pd.concat([X_val, x])\n",
        "    \n",
        "    X_val.drop('id', axis=1, inplace=True)\n",
        "    \n",
        "    print('load X_val'+str(i)+'success')\n",
        "    \n",
        "    X_train = X_train.apply(preprocess)\n",
        "    X_val = X_val.apply(preprocess)\n",
        "    gc.collect()\n",
        "    \n",
        "    print('process_X_success')\n",
        "    \n",
        "    Y_train_all = pd.DataFrame()\n",
        "    for file_path in Y_train_files_names[i]:\n",
        "        y = pd.read_csv(file_path)\n",
        "        Y_train_all = pd.concat([Y_train_all, y])\n",
        "\n",
        "    Y_train_all.drop('id', axis=1, inplace=True)\n",
        "    \n",
        "    print('load Y_train'+str(i)+'success')\n",
        "   \n",
        "    Y_val = pd.DataFrame()\n",
        "    for file_path in Y_val_files_names[i]:\n",
        "        y = pd.read_csv(file_path)\n",
        "        Y_val = pd.concat([Y_val, y])\n",
        "    \n",
        "    print('load Y_val'+str(i)+'success')\n",
        "    \n",
        "    Y_val.drop('id', axis=1, inplace=True)\n",
        "    \n",
        "    os.chdir(\"drive/My Drive/GAL_kaggle\") \n",
        "    X_train.to_csv('subj'+str(i+1)+'_X_train.csv', index=False)\n",
        "    print('convert X_train'+str(i)+'success')\n",
        "    X_val.to_csv('subj'+str(i+1)+'_X_val.csv', index=False)\n",
        "    print('convert X_val'+str(i)+'success')\n",
        "    Y_train_all.to_csv('subj'+str(i+1)+'_Y_train.csv', index=False)\n",
        "    print('convert Y_train'+str(i)+'success')\n",
        "    Y_val.to_csv('subj'+str(i+1)+'_Y_val.csv', index=False)\n",
        "    print('convert Y_val'+str(i)+'success')\n",
        "    os.chdir(\"/content\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load X_train0success\n",
            "load X_val0success\n",
            "process_X_success\n",
            "load Y_train0success\n",
            "load Y_val0success\n",
            "convert X_train0success\n",
            "convert X_val0success\n",
            "convert Y_train0success\n",
            "convert Y_val0success\n",
            "load X_train1success\n",
            "load X_val1success\n",
            "process_X_success\n",
            "load Y_train1success\n",
            "load Y_val1success\n",
            "convert X_train1success\n",
            "convert X_val1success\n",
            "convert Y_train1success\n",
            "convert Y_val1success\n",
            "load X_train2success\n",
            "load X_val2success\n",
            "process_X_success\n",
            "load Y_train2success\n",
            "load Y_val2success\n",
            "convert X_train2success\n",
            "convert X_val2success\n",
            "convert Y_train2success\n",
            "convert Y_val2success\n",
            "load X_train3success\n",
            "load X_val3success\n",
            "process_X_success\n",
            "load Y_train3success\n",
            "load Y_val3success\n",
            "convert X_train3success\n",
            "convert X_val3success\n",
            "convert Y_train3success\n",
            "convert Y_val3success\n",
            "load X_train4success\n",
            "load X_val4success\n",
            "process_X_success\n",
            "load Y_train4success\n",
            "load Y_val4success\n",
            "convert X_train4success\n",
            "convert X_val4success\n",
            "convert Y_train4success\n",
            "convert Y_val4success\n",
            "load X_train5success\n",
            "load X_val5success\n",
            "process_X_success\n",
            "load Y_train5success\n",
            "load Y_val5success\n",
            "convert X_train5success\n",
            "convert X_val5success\n",
            "convert Y_train5success\n",
            "convert Y_val5success\n",
            "load X_train6success\n",
            "load X_val6success\n",
            "process_X_success\n",
            "load Y_train6success\n",
            "load Y_val6success\n",
            "convert X_train6success\n",
            "convert X_val6success\n",
            "convert Y_train6success\n",
            "convert Y_val6success\n",
            "load X_train7success\n",
            "load X_val7success\n",
            "process_X_success\n",
            "load Y_train7success\n",
            "load Y_val7success\n",
            "convert X_train7success\n",
            "convert X_val7success\n",
            "convert Y_train7success\n",
            "convert Y_val7success\n",
            "load X_train8success\n",
            "load X_val8success\n",
            "process_X_success\n",
            "load Y_train8success\n",
            "load Y_val8success\n",
            "convert X_train8success\n",
            "convert X_val8success\n",
            "convert Y_train8success\n",
            "convert Y_val8success\n",
            "load X_train9success\n",
            "load X_val9success\n",
            "process_X_success\n",
            "load Y_train9success\n",
            "load Y_val9success\n",
            "convert X_train9success\n",
            "convert X_val9success\n",
            "convert Y_train9success\n",
            "convert Y_val9success\n",
            "load X_train10success\n",
            "load X_val10success\n",
            "process_X_success\n",
            "load Y_train10success\n",
            "load Y_val10success\n",
            "convert X_train10success\n",
            "convert X_val10success\n",
            "convert Y_train10success\n",
            "convert Y_val10success\n",
            "load X_train11success\n",
            "load X_val11success\n",
            "process_X_success\n",
            "load Y_train11success\n",
            "load Y_val11success\n",
            "convert X_train11success\n",
            "convert X_val11success\n",
            "convert Y_train11success\n",
            "convert Y_val11success\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBTPw8uf0JDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "def ignore_warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqi-nZcmvWIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#训练数据集的generator\n",
        "time_interval = 1024#原数据是500Hz, 取1000个点就是2s的数据\n",
        "subsamples = 4#每间隔100采样点再踩一次样,Tx=30\n",
        "\n",
        "def generator(X, Y, batch_size):\n",
        "    while 1:\n",
        "        x = np.zeros((batch_size, time_interval//subsamples, 32))\n",
        "        y = []\n",
        "        index = np.random.randint(0, len(X)-time_interval-16*batch_size)\n",
        "        indexes = range(index, index + 16*batch_size, 16)\n",
        "        for i , index in enumerate(indexes):\n",
        "\n",
        "            x[i] = X[index : index+time_interval : subsamples]\n",
        "            y.append(Y[index+time_interval])\n",
        "        #x = x.reshape(x.shape[0], x.shape[1], x.shape[2],1)    \n",
        "        y = np.asarray(y)\n",
        "        yield x, y "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfZsUWtpvmlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#validation\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "def val_score(X_val, Y_val, model):\n",
        "    scores = []\n",
        "    num_batch = 10\n",
        "    batch_size = 256\n",
        "\n",
        "    for i in range(num_batch):\n",
        "        x_val = np.zeros((batch_size, time_interval//subsamples, 32))\n",
        "        y_val = []\n",
        "        for i in range(batch_size):\n",
        "            index = np.random.randint(0, X_val.shape[0]-time_interval)\n",
        "            x_val[i] = X_val[index : index+time_interval : subsamples]\n",
        "            y_val.append(Y_val[index+time_interval])\n",
        "        y_val = np.asarray(y_val)\n",
        "        #x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2],1)   \n",
        "        y_out = model.predict(x_val)\n",
        "        score = roc_auc_score(y_val, y_out)\n",
        "        scores.append(score)\n",
        "    scores = np.asarray(scores)\n",
        "\n",
        "    return np.mean(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69lg_Rx6en-r",
        "colab_type": "text"
      },
      "source": [
        "Transfer learning, train the model on each subject"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvtj0BQluSPM",
        "colab_type": "code",
        "outputId": "8c0165d8-3484-4c05-d382-9290bbb39653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "#os.chdir(\"drive/My Drive/GAL_kaggle\")\n",
        "\n",
        "for i in range(12):\n",
        "    X_train = pd.read_csv('subj'+str(i+1)+'_X_train.csv').values\n",
        "    X_val = pd.read_csv('subj'+str(i+1)+'_X_val.csv').values\n",
        "    Y_train = pd.read_csv('subj'+str(i+1)+'_Y_train.csv').values\n",
        "    Y_val = pd.read_csv('subj'+str(i+1)+'_Y_val.csv').values\n",
        "    \n",
        "    \n",
        "    \n",
        "    print('load_subj'+str(i+1)+' data complete!')\n",
        "    model = load_model('new_model_Conv1D.h5')\n",
        "    \n",
        "    savemodel = ModelCheckpoint(filepath='Conv1D_for_subj'+str(i+1)+'.h5')\n",
        "                      \n",
        "    batch_size = 2000\n",
        "    steps_per_epcoh = X_train.shape[0]//batch_size\n",
        "    model.fit_generator(generator(X_train, Y_train, batch_size), steps_per_epoch = steps_per_epcoh ,epochs=2, callbacks=[savemodel], verbose=1)\n",
        "    print('train_model'+str(i+1)+'complete!')\n",
        "                          \n",
        "    score = val_score(X_val, Y_val, model)\n",
        "    print('subj'+str(i+1)+'model AUC scores: '+ str(score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load_subj1 data complete!\n",
            "Epoch 1/2\n",
            "652/652 [==============================] - 92s 141ms/step - loss: 0.0569 - acc: 0.9773\n",
            "Epoch 2/2\n",
            "652/652 [==============================] - 87s 133ms/step - loss: 0.0470 - acc: 0.9799\n",
            "train_model1complete!\n",
            "subj1model AUC scores: 0.9780302496252625\n",
            "load_subj2 data complete!\n",
            "Epoch 1/2\n",
            "780/780 [==============================] - 107s 137ms/step - loss: 0.0703 - acc: 0.9785\n",
            "Epoch 2/2\n",
            "780/780 [==============================] - 107s 137ms/step - loss: 0.0593 - acc: 0.9798\n",
            "train_model2complete!\n",
            "subj2model AUC scores: 0.8534114629718786\n",
            "load_subj3 data complete!\n",
            "Epoch 1/2\n",
            "610/610 [==============================] - 85s 140ms/step - loss: 0.0807 - acc: 0.9728\n",
            "Epoch 2/2\n",
            "610/610 [==============================] - 85s 139ms/step - loss: 0.0680 - acc: 0.9739\n",
            "train_model3complete!\n",
            "subj3model AUC scores: 0.8972959678508733\n",
            "load_subj4 data complete!\n",
            "Epoch 1/2\n",
            "640/640 [==============================] - 89s 139ms/step - loss: 0.0631 - acc: 0.9755\n",
            "Epoch 2/2\n",
            "640/640 [==============================] - 86s 134ms/step - loss: 0.0531 - acc: 0.9775\n",
            "train_model4complete!\n",
            "subj4model AUC scores: 0.950689805757499\n",
            "load_subj5 data complete!\n",
            "Epoch 1/2\n",
            "694/694 [==============================] - 97s 140ms/step - loss: 0.0733 - acc: 0.9754\n",
            "Epoch 2/2\n",
            "694/694 [==============================] - 97s 139ms/step - loss: 0.0610 - acc: 0.9766\n",
            "train_model5complete!\n",
            "subj5model AUC scores: 0.8611807405420526\n",
            "load_subj6 data complete!\n",
            "Epoch 1/2\n",
            "691/691 [==============================] - 92s 133ms/step - loss: 0.0629 - acc: 0.9768\n",
            "Epoch 2/2\n",
            "691/691 [==============================] - 91s 132ms/step - loss: 0.0536 - acc: 0.9782\n",
            "train_model6complete!\n",
            "subj6model AUC scores: 0.9204078890760821\n",
            "load_subj7 data complete!\n",
            "Epoch 1/2\n",
            "756/756 [==============================] - 101s 134ms/step - loss: 0.0616 - acc: 0.9776\n",
            "Epoch 2/2\n",
            "756/756 [==============================] - 100s 133ms/step - loss: 0.0528 - acc: 0.9784\n",
            "train_model7complete!\n",
            "subj7model AUC scores: 0.9350755017707156\n",
            "load_subj8 data complete!\n",
            "Epoch 1/2\n",
            "617/617 [==============================] - 86s 140ms/step - loss: 0.0783 - acc: 0.9727\n",
            "Epoch 2/2\n",
            "617/617 [==============================] - 86s 139ms/step - loss: 0.0656 - acc: 0.9739\n",
            "train_model8complete!\n",
            "subj8model AUC scores: 0.9230850089790967\n",
            "load_subj9 data complete!\n",
            "Epoch 1/2\n",
            "660/660 [==============================] - 93s 141ms/step - loss: 0.0631 - acc: 0.9764\n",
            "Epoch 2/2\n",
            "660/660 [==============================] - 91s 139ms/step - loss: 0.0518 - acc: 0.9791\n",
            "train_model9complete!\n",
            "subj9model AUC scores: 0.923097758173642\n",
            "load_subj10 data complete!\n",
            "Epoch 1/2\n",
            "680/680 [==============================] - 96s 141ms/step - loss: 0.0645 - acc: 0.9755\n",
            "Epoch 2/2\n",
            "680/680 [==============================] - 94s 139ms/step - loss: 0.0550 - acc: 0.9769\n",
            "train_model10complete!\n",
            "subj10model AUC scores: 0.9176012299111951\n",
            "load_subj11 data complete!\n",
            "Epoch 1/2\n",
            "693/693 [==============================] - 97s 140ms/step - loss: 0.0683 - acc: 0.9762\n",
            "Epoch 2/2\n",
            "693/693 [==============================] - 97s 139ms/step - loss: 0.0575 - acc: 0.9772\n",
            "train_model11complete!\n",
            "subj11model AUC scores: 0.9118761830999768\n",
            "load_subj12 data complete!\n",
            "Epoch 1/2\n",
            "726/726 [==============================] - 102s 141ms/step - loss: 0.0673 - acc: 0.9771\n",
            "Epoch 2/2\n",
            "726/726 [==============================] - 101s 139ms/step - loss: 0.0588 - acc: 0.9773\n",
            "train_model12complete!\n",
            "subj12model AUC scores: 0.9200301599836896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0kdFJAce6vT",
        "colab_type": "text"
      },
      "source": [
        "Prepare the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUkarQnfJVY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#pred\n",
        "X_test_files_names = []\n",
        "for subj in range(1, 13):\n",
        "    x_subjs = []\n",
        "    y_subjs = []\n",
        "    for series in range(9, 11):\n",
        "        x_subjs.append('./test/subj'+str(subj)+'_series'+str(series)+'_data.csv')\n",
        "\n",
        "        y_subjs.append('./test/subj'+str(subj)+'_series'+str(series)+'_events.csv')\n",
        "    X_test_files_names.append(x_subjs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfVKOX3-JzBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pred\n",
        "X_test = []\n",
        "\n",
        "for i in range(12):\n",
        "    x_test = pd.DataFrame()\n",
        "    x = pd.read_csv(X_test_files_names[i][0])\n",
        "    x_test = pd.concat([x_test, x])\n",
        "    x = pd.read_csv(X_test_files_names[i][1])\n",
        "    x_test = pd.concat([x_test, x])\n",
        "    x_test.drop('id', axis=1, inplace=True)\n",
        "    x_test = x_test.apply(preprocess)\n",
        "    x_test = x_test.values\n",
        "    X_test.append(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OP6M8xfq-Kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"drive/My Drive/GAL_kaggle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpDZ7hnOJjl-",
        "colab_type": "code",
        "outputId": "f5c7cb34-6f9c-4c69-97d5-72b1cb5718d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#pred\n",
        "from keras.models import load_model\n",
        "time_interval = 1024#原数据是500Hz, 取1000个点就是2s的数据\n",
        "subsamples = 4#每间隔100采样点再踩一次样,Tx=30\n",
        "\n",
        "\n",
        "Y_pred = np.empty([0, 6])\n",
        "for subj in range(12):\n",
        "    y_pred = np.zeros((X_test[subj].shape[0], 6))\n",
        "    pred_batch_size = 128\n",
        "    batch_index = X_test[subj].shape[0]//pred_batch_size\n",
        "    \n",
        "    current_index = 0\n",
        "    model = load_model('Conv1D_for_subj'+ str(subj+1) +'.h5')\n",
        "    for index in range(batch_index):\n",
        "        x_test = np.zeros((pred_batch_size, time_interval//subsamples, 32))\n",
        "        for i in range(pred_batch_size):\n",
        "            if current_index-time_interval < 0:\n",
        "                padding = np.zeros((time_interval-current_index, 32))\n",
        "                x = np.append(padding, X_test[subj][:current_index], axis=0)\n",
        "                x_test[i] = x[::subsamples]\n",
        "            else:    \n",
        "                x_test[i] = X_test[subj][current_index-time_interval : current_index: subsamples]\n",
        "            \n",
        "            current_index += 1\n",
        "        \n",
        "        y_pred[index*pred_batch_size : (index+1)*pred_batch_size] = model.predict(x_test)\n",
        "        \n",
        "        \n",
        "        if index % 50 == 0:\n",
        "            print(index)\n",
        "    \n",
        "    the_rest_batch_size = X_test[subj].shape[0]-batch_index*pred_batch_size\n",
        "    x_test = np.zeros((the_rest_batch_size, time_interval//subsamples, 32))\n",
        "    for i in range(the_rest_batch_size):\n",
        "        x_test[i] = X_test[subj][current_index-time_interval : current_index : subsamples]\n",
        "        current_index += 1\n",
        "    \n",
        "    y_pred[batch_index*pred_batch_size : ] = model.predict(x_test)\n",
        "    \n",
        "    Y_pred = np.append(Y_pred, y_pred, axis=0)\n",
        "    print(X_test[subj].shape[0], current_index, current_index==X_test[subj].shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "233081 233081 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "2200\n",
            "2250\n",
            "2300\n",
            "298085 298085 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "225339 225339 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "244794 244794 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "261060 261060 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "279521 279521 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "277024 277024 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "250264 250264 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "252400 252400 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "257237 257237 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "277497 277497 True\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "2200\n",
            "287869 287869 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZpuNI_z0Bjy",
        "colab_type": "code",
        "outputId": "46c53671-a596-48fe-8125-7e5a25764bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "os.chdir(\"/content\")\n",
        "!unzip sample_submission.csv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYi-4SU_z-Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QepYLD0-0WTu",
        "colab_type": "code",
        "outputId": "1d7ee7b6-17e5-4d2d-9b81-3f7fe3b3e635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "submission.shape[0] "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3144171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_f50xZs0gRI",
        "colab_type": "code",
        "outputId": "d834d5c5-4635-4848-834e-0dca77b7dd3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Y_pred.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3144171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j8TwBww3th9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.loc[:, ['HandStart', 'FirstDigitTouch', 'BothStartLoadPhase', 'LiftOff', 'Replace', 'BothReleased']]=Y_pred\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK4JtLG93-6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv('conv1D_for_subj.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2KOSx3x4ICj",
        "colab_type": "code",
        "outputId": "78fffa1e-039a-41eb-d50e-cbe096dc1159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "!kaggle competitions submit -c grasp-and-lift-eeg-detection -f conv1D_for_subj.csv -m \"Message\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 308M/308M [00:08<00:00, 38.4MB/s]\n",
            "Successfully submitted to Grasp-and-Lift EEG Detection"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}